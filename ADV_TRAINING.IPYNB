{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iit/anaconda3/envs/DPP/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from robustbench import load_model\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=196, shuffle=False, num_workers=4)\n",
    "classes = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading models/cifar10/Linf/Wu2020Adversarial_extra.pt (gdrive_id=1-WJWpAZLlmc4gJ8XXNf7IETjnSZzaCNp).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1-WJWpAZLlmc4gJ8XXNf7IETjnSZzaCNp\n",
      "From (redirected): https://drive.google.com/uc?id=1-WJWpAZLlmc4gJ8XXNf7IETjnSZzaCNp&confirm=t&uuid=7d5e2bda-1441-442b-aee7-beef62f83804\n",
      "To: /home/iit/Downloads/BlindSpots Geometry/models/cifar10/Linf/Wu2020Adversarial_extra.pt\n",
      "100%|██████████| 153M/153M [00:51<00:00, 2.95MB/s] \n"
     ]
    }
   ],
   "source": [
    "def load_model_normalizer(i):\n",
    "    if i==1:\n",
    "        model = load_model(model_name='Standard', dataset='cifar10', threat_model='Linf').to(device)\n",
    "    else:  \n",
    "        model = load_model(model_name='Wu2020Adversarial_extra', dataset='cifar10', threat_model='Linf').to(device) \n",
    "    normalizer = lambda x: x\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    model.eval()\n",
    "    return model, normalizer\n",
    "\n",
    "model, normalizer = load_model_normalizer(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 7 8 8 2 5 2 3 5 0]\n",
      "[0.9999995  0.99975437 0.99999976 0.99999917 0.99998903 0.99953127\n",
      " 0.999998   0.9998977  0.9999825  0.9999721 ]\n"
     ]
    }
   ],
   "source": [
    "# target_classes = [0,1,2,3,4,5,6,7,8,9]\n",
    "image_number = list(range(190, 200))\n",
    "target_images = []\n",
    "to_tensor = transforms.ToTensor()\n",
    "for i, tc in enumerate(image_number):\n",
    "    target_images.append(to_tensor(Image.open(f'./data/CIFAR10/cifar10_target_{tc}.png')))\n",
    "    target_images[-1] = target_images[-1][:3]\n",
    "target_images = torch.stack(target_images, dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = torch.softmax(model(normalizer(target_images.to(device))), dim=-1).cpu().detach().numpy()\n",
    "    target_classes = np.argmax(preds, axis=1)\n",
    "    confidence_levels = np.max(preds, axis=1)\n",
    "    print(target_classes)\n",
    "    print(confidence_levels)\n",
    "    target_classes_str = \"-\".join([str(x) for x in target_classes])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Running attack on examples...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_combine(d1, d2):\n",
    "    for k in d2:\n",
    "        if d2[k] is None:\n",
    "            continue\n",
    "        if k in d1:\n",
    "            if d1[k] is None:\n",
    "                continue\n",
    "            d1[k] = torch.cat((d1[k], d2[k]), dim=0)\n",
    "        else:\n",
    "            d1[k] = d2[k]\n",
    "    return d1\n",
    "\n",
    "def prod(x):\n",
    "    p = 1\n",
    "    for i in x:\n",
    "        p = p*i\n",
    "    return p\n",
    "\n",
    "def get_nullspace_projection(J, v):\n",
    "    y_hat = torch.sum(J*v, -1, keepdim=True)/torch.sum(J * J, -1,  keepdim=True)\n",
    "    x = v - (J * y_hat)\n",
    "    # computing othogonal projection for pert with positive inner product as well\n",
    "    return x\n",
    "\n",
    "def measure_width(img, model, normalizer, index, dir_vec, distances = [5, 10, 15, 20, 25, 30], num_vecs=256):\n",
    "    # print('Entered measure_width')\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "    init_confidence = softmax(model(normalizer(img.clamp(0,1))))[:, index]\n",
    "    rand_vecs = torch.randn(num_vecs, *img.shape[1:]).to(init_confidence.device)\n",
    "    if torch.linalg.vector_norm(dir_vec) == 0:\n",
    "        return np.zeros((len(distances)+1, 2))\n",
    "    dir_vec = dir_vec/torch.linalg.vector_norm(dir_vec)\n",
    "    a = torch.sum(rand_vecs*dir_vec, dim=(1,2,3))\n",
    "    rand_vecs = rand_vecs - torch.einsum(\"ij, jklm -> iklm\" ,a.unsqueeze(-1), dir_vec)\n",
    "    rand_vecs = rand_vecs/torch.linalg.vector_norm(rand_vecs, dim=(1,2,3), keepdim=True)\n",
    "    confs = [torch.Tensor((float(init_confidence), float(init_confidence)))]\n",
    "    for i in distances:\n",
    "        # print(i)\n",
    "        pert_imgs = img + i*rand_vecs\n",
    "        with torch.no_grad():\n",
    "            pert_confs = softmax(model(normalizer(pert_imgs.clamp(0,1))))[:, index]\n",
    "#         min_confs, max_confs = float(torch.min(pert_confs)), float(torch.max(pert_confs))\n",
    "        confs.append(torch.quantile(pert_confs, q=torch.Tensor([0.05, 0.95]).to(pert_confs.device) ).cpu())\n",
    "    confs = torch.stack(confs)\n",
    "    return confs\n",
    "\n",
    "def plot_images(images):\n",
    "    with torch.no_grad():\n",
    "        preds = torch.softmax(model(normalizer(images.to(device))), dim=-1).cpu().detach().numpy()\n",
    "        predicted_classes = np.argmax(preds, axis=1)\n",
    "        confidence_levels = np.max(preds, axis=1)\n",
    "        # print(predicted_classes)\n",
    "        # print(confidence_levels)\n",
    "        \n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for i in range(5):\n",
    "        axes[i].imshow(images[i].permute(1, 2, 0).cpu().detach().numpy())  # Detach tensor before converting to numpy array\n",
    "        axes[i].set_title(classes[predicted_classes[i]], fontsize=10)  # Set title as class name\n",
    "        axes[i].set_xlabel(confidence_levels[i], fontsize=8, labelpad=5, color='blue', ha='center')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_level_set_traversal(model, normalizer, dataset, source_classes, target_image, target_class, stepsize, iterations, device, \n",
    "                           pthresh=0.05, inp=None, get_widths=False, get_confs_over_path=False, get_images=False, get_final_imgs=False, log_step=10, dfunc_list=[]):\n",
    "    source_classes = source_classes.to(device)\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "    if inp is None:\n",
    "        inp = torch.Tensor(dataset).to(device)#.requires_grad_(True)\n",
    "    else:\n",
    "        inp = inp.detach().clone().to(device)#.requires_grad_(True)\n",
    "    mask = torch.ones(*inp.shape, dtype=torch.bool).to(device)\n",
    "    target_image = target_image.to(device)\n",
    "    indices = torch.arange(len(inp))\n",
    "    with torch.no_grad():\n",
    "        pred = model(normalizer(inp.clamp(0,1)))\n",
    "    init_probs = softmax(pred)\n",
    "    # print(source_classes, init_probs[indices, source_classes])\n",
    "    init_labels = torch.argmax(init_probs, dim=-1)\n",
    "    rel_img_mask = (init_labels == source_classes).cpu()\n",
    "    # all_dataset = dataset\n",
    "    # all_source_classes = source_classes\n",
    "    dataset = dataset[rel_img_mask]\n",
    "    source_classes = source_classes[rel_img_mask]\n",
    "    inp = inp[rel_img_mask]\n",
    "    init_probs = init_probs[rel_img_mask]\n",
    "    indices = torch.arange(len(inp))\n",
    "    target_image = target_image[rel_img_mask]\n",
    "\n",
    "    widths_over_path, inp_over_path, confs_over_path, confs_over_path_target = [], [], [], []\n",
    "\n",
    "    adv_delta = None\n",
    "    adv_step_size = 2e-3\n",
    "    width_distances = [0.5, 1, 1.5, 2]\n",
    "    adv_pert = True\n",
    "    all_generated_inputs = []\n",
    "    all_target_classes = []\n",
    "    all_source_classes = []\n",
    "\n",
    "    init_target_layer_acts = source_classes\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    inp = inp + (4/255)*(torch.rand_like(inp)-0.5)\n",
    "    inp = torch.clamp(inp, 0., 1.)\n",
    "    inp.requires_grad_()\n",
    "    print('Started')\n",
    "    for i in range(iterations):\n",
    "        pred = model(normalizer(inp))\n",
    "        probs = softmax(pred)\n",
    "        target_activations = pred\n",
    "        cost = loss(target_activations, init_target_layer_acts)\n",
    "        cost.backward()\n",
    "        J = inp.grad.reshape(len(probs), prod(inp.shape[1:]))\n",
    "        v = target_image.flatten(start_dim=1) - inp.flatten(start_dim=1)\n",
    "        # v = target_image.flatten() - inp.flatten(start_dim=1)\n",
    "        null_vec = get_nullspace_projection(J, v).reshape(inp.shape)\n",
    "\n",
    "        if adv_delta is None:\n",
    "            adv_delta = (adv_step_size/2) * J\n",
    "        else:\n",
    "            adv_delta = adv_delta + (adv_step_size/2) * J\n",
    "        adv_delta = torch.clamp(adv_delta, -adv_step_size, adv_step_size)\n",
    "                \n",
    "        if i%log_step == 0:\n",
    "            # print(f'Iteration {i}')\n",
    "            if get_widths:\n",
    "                # print(f'Getting widths at {i}')\n",
    "                with torch.no_grad():\n",
    "                    widths_at_i = []\n",
    "                    for j in range(len(inp)):\n",
    "                        if probs[j, source_classes[j]] < pthresh:\n",
    "                            widths_at_i.append(np.zeros((len(width_distances)+1,2)))\n",
    "                        else:\n",
    "                            widths_at_i.append(measure_width(inp[j:j+1], model, normalizer, source_classes[j], null_vec[j:j+1], distances=width_distances))\n",
    "                    widths_over_path.append(torch.stack(widths_at_i))\n",
    "            if get_images:\n",
    "                with torch.no_grad():\n",
    "                    inp_over_path.append(inp.detach().cpu())\n",
    "\n",
    "            if get_confs_over_path:\n",
    "                confs_at_i = probs[indices, source_classes].detach().cpu()\n",
    "                confs_at_i_target = probs[indices, target_class].detach().cpu()\n",
    "                confs_over_path.append(confs_at_i)\n",
    "                confs_over_path_target.append(confs_at_i_target)\n",
    "\n",
    "        if adv_pert:\n",
    "            new_inp = inp + stepsize*null_vec - adv_delta.reshape(inp.shape)\n",
    "        else:\n",
    "            new_inp = inp + stepsize*null_vec\n",
    "        new_inp = torch.clamp(new_inp, 0.0, 1.0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_probs = softmax(model(normalizer(new_inp)))\n",
    "\n",
    "        mask = ((init_probs[indices,source_classes] - pred_probs[indices,source_classes] < pthresh)).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        # if ~ (mask.any()):\n",
    "        #     print(\"Iterations stopped at:\", i)\n",
    "        #     break\n",
    "        inp = (inp*(~mask) + new_inp*mask).detach()\n",
    "        inp.requires_grad_()\n",
    "        # if(i%10 == 0):\n",
    "        #     #Plots\n",
    "        #     plot_images(inp)\n",
    "        # Store generated inputs and target classes\n",
    "        if i==0:\n",
    "            for k in range(6):\n",
    "                all_generated_inputs.append(inp.clone().detach())\n",
    "                all_target_classes.append(source_classes.clone().detach())\n",
    "                all_source_classes.append(source_classes.clone().detach())\n",
    "\n",
    "        if i>340:\n",
    "            all_generated_inputs.append(inp.clone().detach())\n",
    "            all_target_classes.append(target_class.clone().detach())\n",
    "            all_source_classes.append(source_classes.clone().detach())\n",
    "    \n",
    "    return all_generated_inputs, all_target_classes, all_source_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:37<00:00, 37.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:39<00:00, 39.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:38<00:00, 38.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:37<00:00, 37.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:37<00:00, 37.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:37<00:00, 37.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:37<00:00, 37.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:37<00:00, 37.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.88s/it]\n"
     ]
    }
   ],
   "source": [
    "example_loader = [(target_images,  torch.LongTensor(target_classes))]\n",
    "iterations = 350\n",
    "all_generated_inputs1 = []\n",
    "all_target_classes1 = []\n",
    "all_source_classes1 = []\n",
    "for i, image in enumerate(target_images):\n",
    "    print(i)\n",
    "    accum_dict = dict([])\n",
    "    for b, data_batch in enumerate(tqdm(example_loader)):\n",
    "        # print(b) 0\n",
    "        batch_target_classes = torch.Tensor([target_classes[i]]*len(data_batch[0])).long()   #[i, i, i, i, i]\n",
    "        batch_target_images = torch.stack([image]*len(data_batch[0]), dim=0)                 #[[image]*5]\n",
    "        generated_inputs, target_classes_all, source_classes_all  = batch_level_set_traversal(model, normalizer, data_batch[0], data_batch[1],\n",
    "                                                    batch_target_images, batch_target_classes, stepsize=1e-2, iterations=iterations, device=device, \n",
    "                                                    pthresh=0.1, get_widths=True, get_images=True, get_final_imgs=True,\n",
    "                                                    dfunc_list=[], get_confs_over_path=True, log_step=50)\n",
    "        all_generated_inputs1.append(generated_inputs)\n",
    "        all_target_classes1.append(target_classes_all)\n",
    "        all_source_classes1.append(source_classes_all)\n",
    "\n",
    "all_generated_inputs1 = [element for sublist in all_generated_inputs1 for element in sublist]\n",
    "all_target_classes1 = [element for sublist in all_target_classes1 for element in sublist]\n",
    "all_source_classes1 = [element for sublist in all_source_classes1 for element in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading models/cifar10/Linf/Wu2020Adversarial_extra.pt (gdrive_id=1-WJWpAZLlmc4gJ8XXNf7IETjnSZzaCNp).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1-WJWpAZLlmc4gJ8XXNf7IETjnSZzaCNp\n",
      "From (redirected): https://drive.google.com/uc?id=1-WJWpAZLlmc4gJ8XXNf7IETjnSZzaCNp&confirm=t&uuid=e5335381-d52c-4851-b2ab-c62f95ba36c8\n",
      "To: /home/iit/Downloads/BlindSpots Geometry/models/cifar10/Linf/Wu2020Adversarial_extra.pt\n",
      "100%|██████████| 153M/153M [02:23<00:00, 1.07MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:03<00:00, 63.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:06<00:00, 66.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:04<00:00, 64.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:04<00:00, 64.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:03<00:00, 63.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m batch_target_classes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([target_classes[i]]\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(data_batch[\u001b[39m0\u001b[39m]))\u001b[39m.\u001b[39mlong()   \u001b[39m#[i, i, i, i, i]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m batch_target_images \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([image]\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(data_batch[\u001b[39m0\u001b[39m]), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)                 \u001b[39m#[[image]*5]\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m generated_inputs, target_classes_all, source_classes_all  \u001b[39m=\u001b[39m batch_level_set_traversal(model, normalizer, data_batch[\u001b[39m0\u001b[39m], data_batch[\u001b[39m1\u001b[39m],\n\u001b[1;32m     16\u001b[0m                                             batch_target_images, batch_target_classes, stepsize\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m, iterations\u001b[39m=\u001b[39miterations, device\u001b[39m=\u001b[39mdevice, \n\u001b[1;32m     17\u001b[0m                                             pthresh\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, get_widths\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, get_images\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, get_final_imgs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m                                             dfunc_list\u001b[39m=\u001b[39m[], get_confs_over_path\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, log_step\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[1;32m     19\u001b[0m all_generated_inputs2\u001b[39m.\u001b[39mappend(generated_inputs)\n\u001b[1;32m     20\u001b[0m all_target_classes2\u001b[39m.\u001b[39mappend(target_classes_all)\n",
      "Cell \u001b[0;32mIn[45], line 71\u001b[0m, in \u001b[0;36mbatch_level_set_traversal\u001b[0;34m(model, normalizer, dataset, source_classes, target_image, target_class, stepsize, iterations, device, pthresh, inp, get_widths, get_confs_over_path, get_images, get_final_imgs, log_step, dfunc_list)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 widths_at_i\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(width_distances)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)))\n\u001b[1;32m     70\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m                 widths_at_i\u001b[39m.\u001b[39mappend(measure_width(inp[j:j\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], model, normalizer, source_classes[j], null_vec[j:j\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], distances\u001b[39m=\u001b[39mwidth_distances))\n\u001b[1;32m     72\u001b[0m         widths_over_path\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mstack(widths_at_i))\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m get_images:\n",
      "Cell \u001b[0;32mIn[44], line 40\u001b[0m, in \u001b[0;36mmeasure_width\u001b[0;34m(img, model, normalizer, index, dir_vec, distances, num_vecs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m distances:\n\u001b[1;32m     38\u001b[0m         \u001b[39m# print(i)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         pert_imgs \u001b[39m=\u001b[39m img \u001b[39m+\u001b[39m i\u001b[39m*\u001b[39mrand_vecs\n\u001b[0;32m---> 40\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     41\u001b[0m             pert_confs \u001b[39m=\u001b[39m softmax(model(normalizer(pert_imgs\u001b[39m.\u001b[39mclamp(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m))))[:, index]\n\u001b[1;32m     42\u001b[0m \u001b[39m#         min_confs, max_confs = float(torch.min(pert_confs)), float(torch.max(pert_confs))\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DPP/lib/python3.11/site-packages/torch/autograd/grad_mode.py:83\u001b[0m, in \u001b[0;36mno_grad.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[1;32m     81\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model, normalizer = load_model_normalizer(2)\n",
    "\n",
    "# example_loader = [(target_images,  torch.LongTensor(target_classes))]\n",
    "# iterations = 500\n",
    "# all_generated_inputs2 = []\n",
    "# all_target_classes2 = []\n",
    "# all_source_classes2 = []\n",
    "# for i, image in enumerate(target_images):\n",
    "#     print(i)\n",
    "#     accum_dict = dict([])\n",
    "#     for b, data_batch in enumerate(tqdm(example_loader)):\n",
    "#         # print(b) 0\n",
    "#         batch_target_classes = torch.Tensor([target_classes[i]]*len(data_batch[0])).long()   #[i, i, i, i, i]\n",
    "#         batch_target_images = torch.stack([image]*len(data_batch[0]), dim=0)                 #[[image]*5]\n",
    "#         generated_inputs, target_classes_all, source_classes_all  = batch_level_set_traversal(model, normalizer, data_batch[0], data_batch[1],\n",
    "#                                                     batch_target_images, batch_target_classes, stepsize=1e-2, iterations=iterations, device=device, \n",
    "#                                                     pthresh=0.1, get_widths=True, get_images=True, get_final_imgs=True,\n",
    "#                                                     dfunc_list=[], get_confs_over_path=True, log_step=50)\n",
    "#         all_generated_inputs2.append(generated_inputs)\n",
    "#         all_target_classes2.append(target_classes_all)\n",
    "#         all_source_classes2.append(source_classes_all)\n",
    "\n",
    "# all_generated_inputs2 = [element for sublist in all_generated_inputs2 for element in sublist]\n",
    "# all_target_classes2 = [element for sublist in all_target_classes2 for element in sublist]\n",
    "# all_source_classes2 = [element for sublist in all_source_classes2 for element in sublist]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340\n"
     ]
    }
   ],
   "source": [
    "# all_generated_inputs = all_generated_inputs1 + all_generated_inputs2\n",
    "# all_target_classes = all_target_classes1 + all_target_classes2\n",
    "# all_source_classes = all_source_classes1 + all_source_classes2\n",
    "# all_generated_inputs = all_generated_inputs1\n",
    "# all_target_classes = all_target_classes1\n",
    "# all_source_classes = all_source_classes1\n",
    "# print(len(all_generated_inputs))\n",
    "\n",
    "# file_path = \"all_generated_inputs.pkl\"\n",
    "# with open(file_path, 'wb') as file:\n",
    "#     pickle.dump(all_generated_inputs, file)\n",
    "\n",
    "# file_path2 = \"all_target_classes.pkl\"\n",
    "# with open(file_path2, 'wb') as file:\n",
    "#     pickle.dump(all_target_classes, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 32, 32])\n",
      "1890\n",
      "torch.Size([18900, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "## load data\n",
    "file_path = \"all_generated_inputs.pkl\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    all_generated_inputs = pickle.load(file)\n",
    "\n",
    "file_path2 = \"all_target_classes.pkl\"\n",
    "with open(file_path2, 'rb') as file:\n",
    "    all_target_classes = pickle.load(file)\n",
    "\n",
    "# #update data\n",
    "# all_generated_inputs = all_generated_inputs + all_generated_inputs1\n",
    "# all_target_classes = all_target_classes + all_target_classes1\n",
    "\n",
    "# #save data\n",
    "# file_path = \"all_generated_inputs.pkl\"\n",
    "# with open(file_path, 'wb') as file:\n",
    "#     pickle.dump(all_generated_inputs, file)\n",
    "\n",
    "# file_path2 = \"all_target_classes.pkl\"\n",
    "# with open(file_path2, 'wb') as file:\n",
    "#     pickle.dump(all_target_classes, file)\n",
    "\n",
    "print(all_generated_inputs[0].shape)\n",
    "print(len(all_target_classes))\n",
    "\n",
    "\n",
    "\n",
    "adversarial_inputs = torch.cat(all_generated_inputs, dim=0)\n",
    "target_class = torch.cat(all_target_classes, dim=0)\n",
    "print(adversarial_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25950, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Define the data augmentation transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.RandomResizedCrop(32, scale=(0.8, 1.0), ratio=(0.8, 1.2)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Randomly select 10,000 indices\n",
    "random_indices = random.sample(range(20000), 12000)\n",
    "\n",
    "# Apply data augmentation to the randomly selected images\n",
    "for idx in random_indices:\n",
    "    img = adversarial_inputs[idx].permute(1, 2, 0).cpu().numpy()  # Convert to numpy array\n",
    "    img = Image.fromarray((img * 255).astype('uint8'))  # Convert to PIL image\n",
    "    img = data_transforms(img)  # Apply transformations\n",
    "    adversarial_inputs[idx] = img\n",
    "\n",
    "print(adversarial_inputs.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training On Adversarial Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training(model, optimizer, criterion, adversarial_inputs, target_class, num_epochs=10, batch_size=196):\n",
    "    model.train()\n",
    "    # adversarial_inputs = torch.cat(adversarial_inputs, dim=0)\n",
    "    # print(adversarial_inputs.shape)\n",
    "    # target_class = torch.cat(target_class, dim=0)\n",
    "    adversarial_inputs.to(device)\n",
    "    target_class.to(device)\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "    subset_indices = np.random.choice(len(train_dataset), 8000, replace=False)\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, subset_indices)\n",
    "    subset_dataloader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(adversarial_inputs, target_class)\n",
    "    # combined_dataset = torch.utils.data.ConcatDataset([train_subset, dataset])\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # adversarial_inputs.requires_grad_()\n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            inputs.requires_grad = True\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if i==0:\n",
    "                outputs_cpu = outputs.cpu().detach()\n",
    "\n",
    "                # Convert to numpy arrays\n",
    "                predicted_classes = np.argmax(outputs_cpu.numpy(), axis=1)\n",
    "                outputs_softmax = F.softmax(outputs, dim=1)\n",
    "\n",
    "                # Convert to numpy arrays\n",
    "                confidence_levels = np.max(outputs_softmax.cpu().detach().numpy(), axis=1)\n",
    "                print(\"Pred\")\n",
    "                print(predicted_classes)\n",
    "                print(\"conf\")\n",
    "                print(confidence_levels)\n",
    "                print(\"tar\")\n",
    "                print(targets)\n",
    "            i= i+1\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss = loss * 50\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "        for inputs, targets in subset_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            inputs.requires_grad = True\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        \n",
    "\n",
    "        # Print statistics\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_accuracy = correct / total\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Pred\n",
      "[6 0 7 5 3 6 3 2 0 1 7 0 2 3 9 1 3 7 0 6 3 9 0 8 1 5 2 3 7 0 3 7 1 4 2 3 1\n",
      " 5 4 3 1 0 0 2 0 3 0 9 3 3 5 6 1 3 9 1 1 1 7 7 2 1 9 1 5 2 1 2 1 9 2 4 3 5\n",
      " 1 1 1 2 5 2 0 4 4 2 9 6 0 9 6 3 6 5 0 6 9 4 5 4 7 7 6 3 1 4 7 2 7 2 9 5 2\n",
      " 3 1 3 1 3 3 9 3 3 2 4 5 6 9 2 3 1]\n",
      "conf\n",
      "[0.38545558 0.5421197  0.54871553 0.33548337 0.28940564 0.508948\n",
      " 0.75017405 0.63591975 0.7326899  0.8823934  0.93732554 0.73833203\n",
      " 0.6059256  0.41117257 0.7834212  0.74626833 0.25208437 0.5485015\n",
      " 0.94066167 0.70484376 0.26739565 0.77859145 0.55133086 0.9496612\n",
      " 0.9332201  0.34038055 0.7084483  0.25434452 0.93810016 0.54436606\n",
      " 0.50304186 0.9378164  0.8760861  0.6559403  0.4043861  0.4839951\n",
      " 0.8772843  0.33773577 0.8881931  0.2547413  0.7377586  0.7387417\n",
      " 0.93525916 0.696376   0.94163644 0.24515855 0.5409731  0.6445433\n",
      " 0.47075462 0.25889808 0.3538518  0.38647583 0.74493724 0.707259\n",
      " 0.7352008  0.88248116 0.7964431  0.76511985 0.7366374  0.89878666\n",
      " 0.63691527 0.80279285 0.6522253  0.77223486 0.7089326  0.86355263\n",
      " 0.764994   0.668325   0.779197   0.76916236 0.50188506 0.5228181\n",
      " 0.4841983  0.578174   0.93443334 0.93370205 0.9348263  0.86472434\n",
      " 0.52898324 0.71094346 0.94132507 0.6558623  0.40412793 0.8511942\n",
      " 0.66876847 0.70693064 0.5425363  0.67374104 0.38731018 0.7420144\n",
      " 0.68765324 0.7044608  0.93875545 0.5054005  0.6292615  0.6623807\n",
      " 0.57014793 0.89427835 0.7428352  0.8997863  0.6670175  0.41138697\n",
      " 0.93402654 0.88769567 0.7602139  0.64520514 0.89877146 0.61999726\n",
      " 0.6487758  0.5850486  0.5067489  0.20301706 0.93491834 0.19423963\n",
      " 0.7390222  0.50194347 0.48437938 0.64524513 0.25191033 0.26105332\n",
      " 0.709317   0.42339006 0.33124575 0.45054    0.5484072  0.84814346\n",
      " 0.41062382 0.76414156]\n",
      "tar\n",
      "tensor([6, 0, 7, 5, 3, 3, 3, 2, 0, 8, 7, 3, 2, 3, 9, 1, 5, 7, 0, 6, 3, 9, 0, 8,\n",
      "        1, 6, 2, 3, 7, 0, 3, 7, 1, 4, 2, 1, 1, 5, 4, 2, 1, 4, 0, 2, 0, 2, 0, 9,\n",
      "        3, 3, 5, 6, 1, 3, 2, 9, 1, 1, 7, 7, 2, 1, 9, 1, 5, 2, 1, 6, 1, 9, 2, 4,\n",
      "        3, 5, 1, 1, 1, 2, 5, 9, 0, 4, 4, 2, 9, 6, 0, 9, 6, 3, 6, 5, 0, 6, 9, 3,\n",
      "        5, 6, 7, 7, 5, 3, 2, 4, 9, 2, 7, 2, 9, 5, 1, 3, 1, 3, 1, 3, 3, 9, 2, 2,\n",
      "        2, 4, 5, 6, 9, 2, 3, 1], device='cuda:0')\n",
      "Epoch [1/500], Loss: 34.3369, Accuracy: 0.8838\n",
      "Epoch [2/500], Loss: 34.3001, Accuracy: 0.8840\n",
      "Epoch [3/500], Loss: 34.3007, Accuracy: 0.8834\n",
      "Epoch [4/500], Loss: 34.3037, Accuracy: 0.8826\n",
      "Epoch [5/500], Loss: 34.3036, Accuracy: 0.8828\n",
      "Epoch [6/500], Loss: 34.3162, Accuracy: 0.8836\n",
      "Epoch [7/500], Loss: 34.3579, Accuracy: 0.8842\n",
      "Epoch [8/500], Loss: 34.3347, Accuracy: 0.8826\n",
      "Epoch [9/500], Loss: 34.3170, Accuracy: 0.8842\n",
      "Epoch [10/500], Loss: 34.3425, Accuracy: 0.8830\n",
      "Epoch [11/500], Loss: 34.3528, Accuracy: 0.8826\n",
      "Epoch [12/500], Loss: 34.3039, Accuracy: 0.8836\n",
      "Epoch [13/500], Loss: 34.3632, Accuracy: 0.8834\n",
      "Epoch [14/500], Loss: 34.3590, Accuracy: 0.8830\n",
      "Epoch [15/500], Loss: 34.2963, Accuracy: 0.8835\n",
      "Epoch [16/500], Loss: 34.3399, Accuracy: 0.8821\n",
      "Epoch [17/500], Loss: 34.3495, Accuracy: 0.8831\n",
      "Epoch [18/500], Loss: 34.3216, Accuracy: 0.8832\n",
      "Epoch [19/500], Loss: 34.3337, Accuracy: 0.8838\n",
      "Epoch [20/500], Loss: 34.3426, Accuracy: 0.8830\n",
      "Epoch [21/500], Loss: 34.3082, Accuracy: 0.8828\n",
      "Epoch [22/500], Loss: 34.3326, Accuracy: 0.8835\n",
      "Epoch [23/500], Loss: 34.3442, Accuracy: 0.8827\n",
      "Epoch [24/500], Loss: 34.3472, Accuracy: 0.8830\n",
      "Epoch [25/500], Loss: 34.3320, Accuracy: 0.8830\n",
      "Epoch [26/500], Loss: 34.3111, Accuracy: 0.8827\n",
      "Epoch [27/500], Loss: 34.3286, Accuracy: 0.8841\n",
      "Epoch [28/500], Loss: 34.3489, Accuracy: 0.8842\n",
      "Epoch [29/500], Loss: 34.3337, Accuracy: 0.8823\n",
      "Epoch [30/500], Loss: 34.3359, Accuracy: 0.8825\n",
      "Epoch [31/500], Loss: 34.2881, Accuracy: 0.8836\n",
      "Epoch [32/500], Loss: 34.3065, Accuracy: 0.8832\n",
      "Epoch [33/500], Loss: 34.2955, Accuracy: 0.8829\n",
      "Epoch [34/500], Loss: 34.3527, Accuracy: 0.8830\n",
      "Epoch [35/500], Loss: 34.3150, Accuracy: 0.8823\n",
      "Epoch [36/500], Loss: 34.3505, Accuracy: 0.8836\n",
      "Epoch [37/500], Loss: 34.3327, Accuracy: 0.8840\n",
      "Epoch [38/500], Loss: 34.3095, Accuracy: 0.8830\n",
      "Epoch [39/500], Loss: 34.3046, Accuracy: 0.8836\n",
      "Epoch [40/500], Loss: 34.3257, Accuracy: 0.8834\n",
      "Epoch [41/500], Loss: 34.3183, Accuracy: 0.8839\n",
      "Epoch [42/500], Loss: 34.3255, Accuracy: 0.8825\n",
      "Epoch [43/500], Loss: 34.3034, Accuracy: 0.8830\n",
      "Epoch [44/500], Loss: 34.3341, Accuracy: 0.8831\n",
      "Epoch [45/500], Loss: 34.3463, Accuracy: 0.8825\n",
      "Epoch [46/500], Loss: 34.3769, Accuracy: 0.8839\n",
      "Epoch [47/500], Loss: 34.3386, Accuracy: 0.8833\n",
      "Epoch [48/500], Loss: 34.3751, Accuracy: 0.8820\n",
      "Epoch [49/500], Loss: 34.3369, Accuracy: 0.8830\n",
      "Epoch [50/500], Loss: 34.3440, Accuracy: 0.8842\n",
      "Epoch [51/500], Loss: 34.3593, Accuracy: 0.8828\n",
      "Epoch [52/500], Loss: 34.3244, Accuracy: 0.8835\n",
      "Epoch [53/500], Loss: 34.3458, Accuracy: 0.8836\n",
      "Epoch [54/500], Loss: 34.2852, Accuracy: 0.8833\n",
      "Epoch [55/500], Loss: 34.3364, Accuracy: 0.8834\n",
      "Epoch [56/500], Loss: 34.3368, Accuracy: 0.8828\n",
      "Epoch [57/500], Loss: 34.3691, Accuracy: 0.8832\n",
      "Epoch [58/500], Loss: 34.3575, Accuracy: 0.8829\n",
      "Epoch [59/500], Loss: 34.3001, Accuracy: 0.8829\n",
      "Epoch [60/500], Loss: 34.2697, Accuracy: 0.8833\n",
      "Epoch [61/500], Loss: 34.3207, Accuracy: 0.8833\n",
      "Epoch [62/500], Loss: 34.3238, Accuracy: 0.8836\n",
      "Epoch [63/500], Loss: 34.3634, Accuracy: 0.8838\n",
      "Epoch [64/500], Loss: 34.3680, Accuracy: 0.8834\n",
      "Epoch [65/500], Loss: 34.3004, Accuracy: 0.8833\n",
      "Epoch [66/500], Loss: 34.3468, Accuracy: 0.8845\n",
      "Epoch [67/500], Loss: 34.2939, Accuracy: 0.8831\n",
      "Epoch [68/500], Loss: 34.3575, Accuracy: 0.8829\n",
      "Epoch [69/500], Loss: 34.3079, Accuracy: 0.8833\n",
      "Epoch [70/500], Loss: 34.3399, Accuracy: 0.8822\n",
      "Epoch [71/500], Loss: 34.3424, Accuracy: 0.8834\n",
      "Epoch [72/500], Loss: 34.2884, Accuracy: 0.8829\n",
      "Epoch [73/500], Loss: 34.3705, Accuracy: 0.8827\n",
      "Epoch [74/500], Loss: 34.3219, Accuracy: 0.8830\n",
      "Epoch [75/500], Loss: 34.2850, Accuracy: 0.8828\n",
      "Epoch [76/500], Loss: 34.3167, Accuracy: 0.8832\n",
      "Epoch [77/500], Loss: 34.2975, Accuracy: 0.8827\n",
      "Epoch [78/500], Loss: 34.2863, Accuracy: 0.8840\n",
      "Epoch [79/500], Loss: 34.3134, Accuracy: 0.8823\n",
      "Epoch [80/500], Loss: 34.3325, Accuracy: 0.8828\n",
      "Epoch [81/500], Loss: 34.3456, Accuracy: 0.8825\n",
      "Epoch [82/500], Loss: 34.3554, Accuracy: 0.8824\n",
      "Epoch [83/500], Loss: 34.3149, Accuracy: 0.8842\n",
      "Epoch [84/500], Loss: 34.2897, Accuracy: 0.8835\n",
      "Epoch [85/500], Loss: 34.3582, Accuracy: 0.8838\n",
      "Epoch [86/500], Loss: 34.3013, Accuracy: 0.8834\n",
      "Epoch [87/500], Loss: 34.3729, Accuracy: 0.8830\n",
      "Epoch [88/500], Loss: 34.3580, Accuracy: 0.8822\n",
      "Epoch [89/500], Loss: 34.3310, Accuracy: 0.8839\n",
      "Epoch [90/500], Loss: 34.3331, Accuracy: 0.8836\n",
      "Epoch [91/500], Loss: 34.3101, Accuracy: 0.8837\n",
      "Epoch [92/500], Loss: 34.3660, Accuracy: 0.8823\n",
      "Epoch [93/500], Loss: 34.3430, Accuracy: 0.8835\n",
      "Epoch [94/500], Loss: 34.3221, Accuracy: 0.8820\n",
      "Epoch [95/500], Loss: 34.3348, Accuracy: 0.8836\n",
      "Epoch [96/500], Loss: 34.3275, Accuracy: 0.8838\n",
      "Epoch [97/500], Loss: 34.3619, Accuracy: 0.8838\n",
      "Epoch [98/500], Loss: 34.3296, Accuracy: 0.8833\n",
      "Epoch [99/500], Loss: 34.3837, Accuracy: 0.8823\n",
      "Epoch [100/500], Loss: 34.2923, Accuracy: 0.8833\n",
      "Epoch [101/500], Loss: 34.3826, Accuracy: 0.8830\n",
      "Epoch [102/500], Loss: 34.3262, Accuracy: 0.8829\n",
      "Epoch [103/500], Loss: 34.3606, Accuracy: 0.8823\n",
      "Epoch [104/500], Loss: 34.3443, Accuracy: 0.8823\n",
      "Epoch [105/500], Loss: 34.2961, Accuracy: 0.8826\n",
      "Epoch [106/500], Loss: 34.3041, Accuracy: 0.8842\n",
      "Epoch [107/500], Loss: 34.3068, Accuracy: 0.8836\n",
      "Epoch [108/500], Loss: 34.3282, Accuracy: 0.8823\n",
      "Epoch [109/500], Loss: 34.3319, Accuracy: 0.8834\n",
      "Epoch [110/500], Loss: 34.2951, Accuracy: 0.8832\n",
      "Epoch [111/500], Loss: 34.3562, Accuracy: 0.8830\n",
      "Epoch [112/500], Loss: 34.3459, Accuracy: 0.8833\n",
      "Epoch [113/500], Loss: 34.3093, Accuracy: 0.8830\n",
      "Epoch [114/500], Loss: 34.3520, Accuracy: 0.8838\n",
      "Epoch [115/500], Loss: 34.3468, Accuracy: 0.8836\n",
      "Epoch [116/500], Loss: 34.3505, Accuracy: 0.8832\n",
      "Epoch [117/500], Loss: 34.3226, Accuracy: 0.8833\n",
      "Epoch [118/500], Loss: 34.3152, Accuracy: 0.8833\n",
      "Epoch [119/500], Loss: 34.3272, Accuracy: 0.8843\n",
      "Epoch [120/500], Loss: 34.3401, Accuracy: 0.8820\n",
      "Epoch [121/500], Loss: 34.3655, Accuracy: 0.8832\n",
      "Epoch [122/500], Loss: 34.3435, Accuracy: 0.8830\n",
      "Epoch [123/500], Loss: 34.3502, Accuracy: 0.8833\n",
      "Epoch [124/500], Loss: 34.2923, Accuracy: 0.8839\n",
      "Epoch [125/500], Loss: 34.3098, Accuracy: 0.8823\n",
      "Epoch [126/500], Loss: 34.3314, Accuracy: 0.8833\n",
      "Epoch [127/500], Loss: 34.3404, Accuracy: 0.8842\n",
      "Epoch [128/500], Loss: 34.3113, Accuracy: 0.8831\n",
      "Epoch [129/500], Loss: 34.3394, Accuracy: 0.8828\n",
      "Epoch [130/500], Loss: 34.3180, Accuracy: 0.8825\n",
      "Epoch [131/500], Loss: 34.3309, Accuracy: 0.8833\n",
      "Epoch [132/500], Loss: 34.3618, Accuracy: 0.8830\n",
      "Epoch [133/500], Loss: 34.3347, Accuracy: 0.8830\n",
      "Epoch [134/500], Loss: 34.3595, Accuracy: 0.8836\n",
      "Epoch [135/500], Loss: 34.3643, Accuracy: 0.8828\n",
      "Epoch [136/500], Loss: 34.3337, Accuracy: 0.8837\n",
      "Epoch [137/500], Loss: 34.3820, Accuracy: 0.8829\n",
      "Epoch [138/500], Loss: 34.3084, Accuracy: 0.8829\n",
      "Epoch [139/500], Loss: 34.2998, Accuracy: 0.8832\n",
      "Epoch [140/500], Loss: 34.3190, Accuracy: 0.8840\n",
      "Epoch [141/500], Loss: 34.3607, Accuracy: 0.8835\n",
      "Epoch [142/500], Loss: 34.3088, Accuracy: 0.8828\n",
      "Epoch [143/500], Loss: 34.3115, Accuracy: 0.8831\n",
      "Epoch [144/500], Loss: 34.3676, Accuracy: 0.8837\n",
      "Epoch [145/500], Loss: 34.3432, Accuracy: 0.8826\n",
      "Epoch [146/500], Loss: 34.3522, Accuracy: 0.8826\n",
      "Epoch [147/500], Loss: 34.3038, Accuracy: 0.8827\n",
      "Epoch [148/500], Loss: 34.3542, Accuracy: 0.8827\n",
      "Epoch [149/500], Loss: 34.3738, Accuracy: 0.8827\n",
      "Epoch [150/500], Loss: 34.3420, Accuracy: 0.8834\n",
      "Epoch [151/500], Loss: 34.3267, Accuracy: 0.8833\n",
      "Epoch [152/500], Loss: 34.3531, Accuracy: 0.8829\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[165], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m      4\u001b[0m criterion \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 6\u001b[0m adversarial_training(model, optimizer, criterion, adversarial_inputs, target_class, num_epochs\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n",
      "Cell \u001b[0;32mIn[164], line 53\u001b[0m, in \u001b[0;36madversarial_training\u001b[0;34m(model, optimizer, criterion, adversarial_inputs, target_class, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     52\u001b[0m \u001b[39m# Compute statistics\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     54\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, normalizer = load_model_normalizer(2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "adversarial_training(model, optimizer, criterion, adversarial_inputs, target_class, num_epochs=500, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Wu2020Adversarial_extra_trained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4 (main, Jul  5 2023, 14:15:25) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40aab00612181d8a98b512b1d0f6e983224156b37e4693a55d0dbba4659947eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
